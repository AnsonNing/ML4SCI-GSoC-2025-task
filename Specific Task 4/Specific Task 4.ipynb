{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors, BallTree, kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.nn import SAGEConv, BatchNorm, LayerNorm, GATv2Conv, global_mean_pool, global_max_pool, GATConv, GCNConv\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import GlobalAttention\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "h5_file = 'quark-gluon_data-set_n139306.hdf5'\n",
    "\n",
    "with h5py.File(h5_file, 'r') as f:\n",
    "    X_jets = f['X_jets'][:]  \n",
    "    y = f['y'][:].astype(np.int64) \n",
    "\n",
    "print(\"X_jets shape:\", X_jets.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, batch):\n",
    "\n",
    "    node_slice = torch.cumsum(torch.from_numpy(np.bincount(batch)), 0)\n",
    "    node_slice = torch.cat([torch.tensor([0]), node_slice])\n",
    "    data.__num_nodes__ = torch.bincount(batch).tolist()\n",
    "\n",
    "    slices = {\n",
    "        'x': node_slice,\n",
    "        'y': torch.tensor([0], dtype=torch.long),\n",
    "        'edge_index': torch.tensor([0], dtype=torch.long),\n",
    "        'edge_attr': torch.tensor([0], dtype=torch.long)\n",
    "    }\n",
    "\n",
    "    if data.y is not None:\n",
    "        if data.y.size(0) == batch.size(0):\n",
    "            slices['y'] = node_slice\n",
    "        else:\n",
    "            slices['y'] = torch.arange(0, batch[-1] + 2, dtype=torch.long)\n",
    "\n",
    "    return data, slices\n",
    "\n",
    "scaler_ecal = StandardScaler()\n",
    "scaler_hcal = StandardScaler()\n",
    "scaler_tracks = StandardScaler()\n",
    "\n",
    "def fit_global_scalers(X_jets):\n",
    "\n",
    "    global scaler_ecal, scaler_hcal, scaler_tracks\n",
    "    \n",
    "    all_points = []\n",
    "\n",
    "    for img in tqdm(X_jets):\n",
    "        mask = np.any(img > 1e-3, axis=2) \n",
    "        y_coords, x_coords = np.nonzero(mask)  \n",
    "        \n",
    "        if len(y_coords) > 0:  \n",
    "            all_points.append(img[y_coords, x_coords, :])  \n",
    "\n",
    "    all_points = np.vstack(all_points)\n",
    "\n",
    "    scaler_ecal.fit(all_points[:, 0].reshape(-1, 1))\n",
    "    scaler_hcal.fit(all_points[:, 1].reshape(-1, 1))\n",
    "    scaler_tracks.fit(all_points[:, 2].reshape(-1, 1))\n",
    "    \n",
    "    print(\"Global scalers fitted!\")\n",
    "\n",
    "def normalize_point_cloud(points):\n",
    "\n",
    "    points_norm = np.copy(points)\n",
    "    points_norm[:, 0] = scaler_ecal.transform(points[:, 0].reshape(-1, 1)).flatten()\n",
    "    points_norm[:, 1] = scaler_hcal.transform(points[:, 1].reshape(-1, 1)).flatten()\n",
    "    points_norm[:, 2] = scaler_tracks.transform(points[:, 2].reshape(-1, 1)).flatten()\n",
    "    return points_norm\n",
    "\n",
    "def image_to_point_cloud(image):\n",
    "    mask = np.sum(image, axis=2) > 0 \n",
    "    y_coords, x_coords = np.nonzero(mask) \n",
    "    features = image[y_coords, x_coords, :] \n",
    "    points = np.hstack((features, x_coords[:, None], y_coords[:, None]))  \n",
    "    points = normalize_point_cloud(points)  \n",
    "\n",
    "    return points.astype(np.float32)\n",
    "\n",
    "def point_cloud_to_graph(points, k=5):\n",
    "\n",
    "    num_nodes = points.shape[0]\n",
    "\n",
    "    k_eff = max(2, min(k + 1, num_nodes))\n",
    "\n",
    "    tree = BallTree(points[:, -2:])  \n",
    "    distances, indices = tree.query(points[:, -2:], k=k_eff)\n",
    "\n",
    "    neighbors = indices[:, 1:] \n",
    "    \n",
    "    delta_features = points[neighbors, :-2] - points[:, None, :-2]\n",
    "    delta_features = delta_features.reshape(-1, 3)\n",
    "\n",
    "    dist_vals = distances[:, 1:].reshape(-1, 1)  \n",
    "\n",
    "    edge_attr = np.hstack((dist_vals, delta_features)).astype(np.float32)\n",
    "\n",
    "    source_nodes = np.repeat(np.arange(num_nodes), k_eff - 1) \n",
    "    edge_index = np.stack((source_nodes, neighbors.reshape(-1)), axis=0).astype(np.int32)\n",
    "\n",
    "    return points, edge_index, edge_attr\n",
    "\n",
    "def read_graph(X_jets, y, k=5):\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "    node_graph_id_list = []\n",
    "    y_list = []\n",
    "\n",
    "    num_nodes_list = []\n",
    "    num_edges_list = []\n",
    "\n",
    "    for img_idx, img in enumerate(tqdm(X_jets)):\n",
    "        points = image_to_point_cloud(img)\n",
    "\n",
    "        vertices, img_edge_index, img_edge_attr = point_cloud_to_graph(points, k=k)\n",
    "        x_list.append(vertices)\n",
    "        edge_index_list.append(img_edge_index)\n",
    "        edge_attr_list.append(img_edge_attr)\n",
    "        node_graph_id_list.append(np.full(vertices.shape[0], img_idx, dtype=np.int32))\n",
    "        y_list.append(y[img_idx].reshape(1, -1))\n",
    "\n",
    "        num_nodes_list.append(vertices.shape[0])\n",
    "        num_edges_list.append(img_edge_index.shape[1])\n",
    "\n",
    "    x = np.vstack(x_list) \n",
    "    edge_index = np.hstack(edge_index_list) \n",
    "    edge_attr = np.vstack(edge_attr_list)  \n",
    "    node_graph_id = np.concatenate(node_graph_id_list) \n",
    "    y_data = np.vstack(y_list) \n",
    "\n",
    "    x = torch.from_numpy(x).to(torch.float32).pin_memory()\n",
    "    edge_index = torch.from_numpy(edge_index).to(torch.int64).pin_memory()\n",
    "    edge_attr = torch.from_numpy(edge_attr).to(torch.float32).pin_memory()\n",
    "    y_data = torch.from_numpy(y_data).to(torch.float32).pin_memory()\n",
    "    node_graph_id = torch.from_numpy(node_graph_id).to(torch.int64).pin_memory()\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_data)\n",
    "    \n",
    "    data, slices = split(data, node_graph_id)\n",
    "\n",
    "    edge_slice = np.concatenate(([0], np.cumsum(num_edges_list)))  \n",
    "    slices['edge_index'] = torch.from_numpy(edge_slice).to(torch.int32)\n",
    "    slices['edge_attr'] = torch.from_numpy(edge_slice).to(torch.int32)\n",
    "\n",
    "    return data, slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = \"jet_graphs.pkl\"\n",
    "\n",
    "if os.path.exists(graph_file):\n",
    "    with open(graph_file, \"rb\") as f:\n",
    "        data, slices = pickle.load(f)\n",
    "    print(\"Loaded preprocessed graphs from file!\")\n",
    "else:   \n",
    "    fit_global_scalers(X_jets)\n",
    "    data, slices = read_graph(X_jets, y, k=5)\n",
    "    \n",
    "    with open(graph_file, \"wb\") as f:\n",
    "        pickle.dump((data, slices), f)\n",
    "    print(\"Graph dataset processed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, graph_file, transform=None, pre_transform=None):\n",
    "        super(JetGraphDataset, self).__init__(None, transform, pre_transform)\n",
    "    \n",
    "        if os.path.exists(graph_file):\n",
    "            with open(graph_file, \"rb\") as f:\n",
    "                loaded = pickle.load(f)\n",
    "            \n",
    "            assert isinstance(loaded, tuple) and len(loaded) == 2\n",
    "            \n",
    "            self.data, self.slices = loaded \n",
    "\n",
    "    def len(self):\n",
    "        assert self.slices is not None\n",
    "        return len(self.slices['x']) - 1 \n",
    "\n",
    "dataset = JetGraphDataset(graph_file=graph_file)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(dataset)), test_size=0.2, stratify=dataset.data.y.numpy()\n",
    ")\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_idx, test_size=0.125, stratify=dataset.data.y.numpy()[train_idx]\n",
    ")\n",
    "\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) #64\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch x shape: {batch.x.shape}\")\n",
    "    print(f\"Batch edge_index shape: {batch.edge_index.shape}\")\n",
    "    print(f\"Batch y shape: {batch.y.shape}\")\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.gcn1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.gcn2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Hybrid_GATv2_GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, heads=[8, 6, 6], dropout=0.5):\n",
    "        super(Hybrid_GATv2_GCN, self).__init__()\n",
    "\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads[0], dropout=dropout)\n",
    "        self.bn1 = LayerNorm(hidden_channels * heads[0])\n",
    "\n",
    "        self.gat2 = GATv2Conv(hidden_channels * heads[0], hidden_channels, heads=heads[1], dropout=dropout)\n",
    "        self.bn2 = LayerNorm(hidden_channels * heads[1])\n",
    "\n",
    "        self.gcn = GCNConv(hidden_channels * heads[1], hidden_channels)\n",
    "        self.bn3 = LayerNorm(hidden_channels)\n",
    "\n",
    "        self.gat3 = GATv2Conv(hidden_channels, hidden_channels, heads=heads[2], dropout=dropout)\n",
    "        self.bn4 = LayerNorm(hidden_channels * heads[2])\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels * heads[2], num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.gcn(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.gat3(x, edge_index)\n",
    "        x = self.bn4(x)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "in_channels = dataset.data.x.shape[1]\n",
    "hidden_channels = 128\n",
    "num_classes = len(torch.unique(dataset.data.y))\n",
    "\n",
    "baseline_model = GCN(in_channels, hidden_channels, num_classes).to(device)\n",
    "hybrid_model = Hybrid_GATv2_GCN(in_channels, hidden_channels, num_classes).to(device)\n",
    "\n",
    "optimizer_baseline = torch.optim.Adam(baseline_model.parameters(), lr=1e-3, weight_decay=5e-5)\n",
    "optimizer_hybrid = torch.optim.Adam(hybrid_model.parameters(), lr=1e-3, weight_decay=5e-5)\n",
    "scheduler_baseline = CosineAnnealingLR(optimizer_baseline, T_max=50, eta_min=1e-5)\n",
    "scheduler_hybrid = CosineAnnealingLR(optimizer_hybrid, T_max=50, eta_min=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y.view(-1).long())\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y.view(-1)).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y.view(-1).long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y.view(-1)).sum().item()\n",
    "            total += data.y.size(0)\n",
    "\n",
    "            all_preds.append(F.softmax(out, dim=1)[:, 1].cpu().numpy()) \n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    return total_loss / len(loader), correct / total, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "val_aucs_baseline, val_aucs_hybrid = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(baseline_model, optimizer_baseline, train_loader)\n",
    "    train(hybrid_model, optimizer_hybrid, train_loader)\n",
    "\n",
    "    val_loss_baseline, val_acc_baseline, auc_baseline = evaluate(baseline_model, val_loader)\n",
    "    val_loss_hybrid, val_acc_hybrid, auc_hybrid = evaluate(hybrid_model, val_loader)\n",
    "\n",
    "    val_aucs_baseline.append(auc_baseline)\n",
    "    val_aucs_hybrid.append(auc_hybrid)\n",
    "\n",
    "    scheduler_baseline.step()\n",
    "    scheduler_hybrid.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"GCN - AUC: {auc_baseline:.4f} | \"\n",
    "          f\"Hybrid GATv2-GCN - AUC: {auc_hybrid:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), val_aucs_baseline, label=\"GCN ROC-AUC\", color='blue', linewidth=1.5)\n",
    "plt.plot(range(1, num_epochs + 1), val_aucs_hybrid, label=\"Hybrid Validation ROC-AUC\", color='red', linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.title(\"ROC-AUC Comparison between GCN and Hybrid GATv2-GCN\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "test_loss_baseline, test_acc_baseline, test_auc_baseline = evaluate(baseline_model, test_loader)\n",
    "test_loss_hybrid, test_acc_hybrid, test_auc_hybrid = evaluate(hybrid_model, test_loader)\n",
    "\n",
    "print(f\"\\nFinal Test ROC-AUC - GCN: {test_auc_baseline:.4f}, Hybrid GATv2-GCN: {test_auc_hybrid:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
